"""
AstrBot Server Status Plugin v4.0 - ä¼ä¸šçº§å¢å¼ºç‰ˆ
å·¥ä¸šçº§æœåŠ¡å™¨çŠ¶æ€ç›‘æ§æ’ä»¶ï¼Œæ”¯æŒæ™ºèƒ½ç¼“å­˜ã€å¹¶è¡Œé‡‡é›†å’Œå¥åº·æ£€æŸ¥
"""

import asyncio
import datetime
import logging
import os
import platform
import psutil
import time
import math
import statistics
from enum import Enum, auto
from typing import Dict, List, Optional, Set, Tuple, Union, Any, Deque
from dataclasses import dataclass, field
from collections import deque
from concurrent.futures import ThreadPoolExecutor, as_completed

from astrbot.api.star import Star, register, Context
from astrbot.api.event import filter as event_filter, AstrMessageEvent
from astrbot.api import logger as astr_logger, AstrBotConfig

# é…ç½®æ’ä»¶æ—¥å¿—
logger = logging.getLogger(__name__)

# --- æšä¸¾å’Œå¸¸é‡å®šä¹‰ ---

class CacheLevel(Enum):
    """ç¼“å­˜çº§åˆ«"""
    FULL_SUCCESS = auto()    # å®Œå…¨æˆåŠŸ
    PARTIAL_SUCCESS = auto() # éƒ¨åˆ†æˆåŠŸ  
    FAILED = auto()          # å®Œå…¨å¤±è´¥

class ErrorSeverity(Enum):
    """é”™è¯¯ä¸¥é‡çº§åˆ«"""
    WARNING = auto()    # è­¦å‘Šï¼Œå¯ç»§ç»­ä½¿ç”¨ç¼“å­˜
    ERROR = auto()      # é”™è¯¯ï¼Œéœ€è¦é‡æ–°é‡‡é›†
    CRITICAL = auto()   # ä¸¥é‡é”™è¯¯ï¼Œåœæ­¢æœåŠ¡

# --- æ•°æ®å¥‘çº¦å®šä¹‰ ---

@dataclass(frozen=True)
class DiskUsage:
    """ç£ç›˜ä½¿ç”¨æƒ…å†µ"""
    display_path: str
    total: int
    used: int
    free: int
    percent: float
    is_critical: bool = field(default=False)  # æ˜¯å¦å…³é”®ç£ç›˜

@dataclass(frozen=True)
class SystemMetrics:
    """ç³»ç»ŸæŒ‡æ ‡å¿«ç…§"""
    cpu_percent: Optional[float]
    cpu_temp: Optional[float]
    mem_total: Optional[int]
    mem_used: Optional[int]
    mem_percent: Optional[float]
    net_sent: Optional[int]
    net_recv: Optional[int]
    uptime: Optional[datetime.timedelta]
    is_container_uptime: bool
    disks: Optional[List[DiskUsage]]
    process_count: Optional[int] = field(default=None)  # è¿›ç¨‹æ•°é‡
    load_avg: Optional[float] = field(default=None)    # ç³»ç»Ÿè´Ÿè½½
    errors: List[Tuple[str, ErrorSeverity]] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    cache_level: CacheLevel = field(default=CacheLevel.FULL_SUCCESS)

# --- å·¥å…·å‡½æ•° ---

def get_optimal_thread_count() -> int:
    """è·å–æœ€ä¼˜çº¿ç¨‹æ•°"""
    cpu_count = os.cpu_count() or 4
    return max(1, min(cpu_count - 1, 8))  # é™åˆ¶æœ€å¤§8çº¿ç¨‹

def calculate_cache_duration(errors: List[Tuple[str, ErrorSeverity]]) -> int:
    """æ ¹æ®é”™è¯¯ä¸¥é‡ç¨‹åº¦è®¡ç®—ç¼“å­˜æ—¶é—´"""
    if not errors:
        return 10  # é»˜è®¤10ç§’
    
    severities = [sev for _, sev in errors]
    if ErrorSeverity.CRITICAL in severities:
        return 2   # ä¸¥é‡é”™è¯¯æ—¶ç¼©çŸ­ç¼“å­˜
    elif ErrorSeverity.ERROR in severities:
        return 5   # æ™®é€šé”™è¯¯
    else:
        return 8   # åªæœ‰è­¦å‘Šæ—¶ç¨çŸ­ç¼“å­˜

def safe_disk_path(path: Any) -> bool:
    """æ£€æŸ¥ç£ç›˜è·¯å¾„æ˜¯å¦å®‰å…¨"""
    if not isinstance(path, str) or not path or len(path) > 1024:
        return False
        
    normalized = path.replace('\\', '/')
    
    # å®‰å…¨æ£€æŸ¥
    forbidden = ['..', '~', '\0', '*', '?', '|', '<', '>', '"', '//', '\\\\']
    if any(pattern in path for pattern in forbidden):
        return False
        
    if '../' in normalized or '/..' in normalized:
        return False
        
    # Windowsç‰¹å®šæ£€æŸ¥
    if platform.system() == "Windows":
        if normalized.startswith('//') and '..' in normalized:
            return False
        if ':' in path and path.index(':') > 1:
            return False
            
    return os.path.isabs(path)

def is_running_in_container() -> bool:
    """æ£€æµ‹æ˜¯å¦è¿è¡Œåœ¨å®¹å™¨ä¸­"""
    # å®¹å™¨æ£€æµ‹é€»è¾‘
    indicators = ['/.dockerenv', '/.dockerinit']
    
    # æ£€æŸ¥æ–‡ä»¶ç³»ç»ŸæŒ‡ç¤ºå™¨
    for indicator in indicators:
        if os.path.exists(indicator):
            return True
            
    # æ£€æŸ¥cgroup
    if platform.system() != "Windows":
        try:
            cgroup_paths = ['/proc/1/cgroup', '/proc/self/cgroup']
            for cgroup_path in cgroup_paths:
                if os.path.exists(cgroup_path):
                    with open(cgroup_path, 'rt', encoding='utf-8') as f:
                        content = f.read()
                        if any(keyword in content for keyword in ['docker', 'kubepods', 'containerd', 'lxc']):
                            return True
        except (FileNotFoundError, PermissionError, UnicodeDecodeError):
            pass
            
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    env_keys = os.environ.keys()
    container_env_vars = ['KUBERNETES_SERVICE_HOST', 'DOCKER_CONTAINER']
    if any(key in env_keys for key in container_env_vars):
        return True
        
    return False

# --- æ ¸å¿ƒç»„ä»¶ ---

class MetricsCollector:
    """ç³»ç»ŸæŒ‡æ ‡é‡‡é›†å™¨ - æ”¯æŒå¹¶è¡Œé‡‡é›†å’Œæ™ºèƒ½é‡è¯•"""
    
    MAX_DISK_COUNT = 20  # å¢åŠ æœ€å¤§ç£ç›˜æ•°é‡
    CPU_TEMP_KEYS = ['coretemp', 'k10temp', 'cpu_thermal', 'acpitz', 'zenpower']
    IGNORED_FS_TYPES = {'nfs', 'nfs4', 'smbfs', 'cifs', 'tmpfs', 'devtmpfs', 'proc', 'sysfs'}
    MAX_RETRY_ATTEMPTS = 2
    RETRY_DELAY = 0.1  # 100msé‡è¯•å»¶è¿Ÿ

    def __init__(self, disk_config: List[Dict[str, str]], show_temp: bool):
        self.disk_config = disk_config
        self.show_temp = show_temp
        self.boot_time, self.is_container_uptime = self._get_boot_time()
        self.optimal_threads = get_optimal_thread_count()
        self.executor = ThreadPoolExecutor(max_workers=self.optimal_threads)
        self._historical_metrics: Deque[SystemMetrics] = deque(maxlen=60)  # ä¿å­˜æœ€è¿‘60æ¬¡é‡‡é›†

    def _get_boot_time(self) -> Tuple[Optional[datetime.datetime], bool]:
        """è·å–å¯åŠ¨æ—¶é—´"""
        try:
            if is_running_in_container():
                proc_one_creation = psutil.Process(1).create_time()
                return datetime.datetime.fromtimestamp(proc_one_creation), True
        except (psutil.Error, FileNotFoundError, PermissionError):
            pass
            
        try:
            return datetime.datetime.fromtimestamp(psutil.boot_time()), False
        except Exception as e:
            logger.error("Failed to get boot time: %s", e)
            return None, False

    async def collect_metrics(self) -> SystemMetrics:
        """å¼‚æ­¥æ”¶é›†ç³»ç»ŸæŒ‡æ ‡ - æ”¯æŒæ™ºèƒ½é‡è¯•"""
        for attempt in range(self.MAX_RETRY_ATTEMPTS + 1):
            try:
                metrics = await asyncio.get_event_loop().run_in_executor(
                    self.executor, self._collect_sync
                )
                self._historical_metrics.append(metrics)
                return metrics
            except Exception as e:
                if attempt == self.MAX_RETRY_ATTEMPTS:
                    logger.error("Metrics collection failed after %d attempts: %s", 
                                self.MAX_RETRY_ATTEMPTS + 1, e)
                    error_metrics = SystemMetrics(
                        cpu_percent=None, cpu_temp=None, mem_total=None, mem_used=None,
                        mem_percent=None, net_sent=None, net_recv=None, uptime=None,
                        is_container_uptime=False, disks=[], 
                        errors=[(f"Collection failed after {self.MAX_RETRY_ATTEMPTS + 1} attempts: {str(e)}", 
                                ErrorSeverity.CRITICAL)],
                        cache_level=CacheLevel.FAILED
                    )
                    self._historical_metrics.append(error_metrics)
                    return error_metrics
                
                logger.warning("Metrics collection attempt %d failed, retrying...: %s", 
                              attempt + 1, e)
                await asyncio.sleep(self.RETRY_DELAY * (attempt + 1))

    def _collect_sync(self) -> SystemMetrics:
        """åŒæ­¥æ”¶é›†ç³»ç»ŸæŒ‡æ ‡ - æ”¯æŒå¹¶è¡Œé‡‡é›†"""
        errors: List[Tuple[str, ErrorSeverity]] = []
        warnings = []
        
        # å¹¶è¡Œé‡‡é›†åŸºç¡€æŒ‡æ ‡
        with ThreadPoolExecutor(max_workers=min(4, self.optimal_threads)) as executor:
            futures = {
                executor.submit(self._collect_cpu_metrics): 'cpu',
                executor.submit(self._collect_memory_metrics): 'memory',
                executor.submit(self._collect_network_metrics): 'network',
                executor.submit(self._collect_system_metrics): 'system'
            }
            
            results = {}
            for future in as_completed(futures):
                metric_type = futures[future]
                try:
                    results[metric_type] = future.result()
                except Exception as e:
                    logger.warning("Failed to collect %s metrics: %s", metric_type, e)
                    errors.append((f"{metric_type.capitalize()}é‡‡é›†å¤±è´¥", ErrorSeverity.ERROR))

        # åˆå¹¶é‡‡é›†ç»“æœ
        cpu_p, cpu_t = results.get('cpu', (None, None))
        mem_data = results.get('memory')
        net_data = results.get('network')
        process_count, load_avg = results.get('system', (None, None))

        # å¹¶è¡Œé‡‡é›†ç£ç›˜ä½¿ç”¨æƒ…å†µ
        disks = self._get_disk_usages_parallel(errors)
        
        uptime = (datetime.datetime.now() - self.boot_time) if self.boot_time else None
        
        # ç¡®å®šç¼“å­˜çº§åˆ«
        cache_level = self._determine_cache_level(errors, disks)
        
        return SystemMetrics(
            cpu_percent=cpu_p, cpu_temp=cpu_t,
            mem_total=getattr(mem_data, 'total', None) if mem_data else None,
            mem_used=getattr(mem_data, 'used', None) if mem_data else None,
            mem_percent=getattr(mem_data, 'percent', None) if mem_data else None,
            net_sent=getattr(net_data, 'bytes_sent', None) if net_data else None,
            net_recv=getattr(net_data, 'bytes_recv', None) if net_data else None,
            uptime=uptime, is_container_uptime=self.is_container_uptime,
            disks=disks, process_count=process_count, load_avg=load_avg,
            errors=errors, warnings=warnings, cache_level=cache_level
        )

    def _collect_cpu_metrics(self) -> Tuple[Optional[float], Optional[float]]:
        """é‡‡é›†CPUæŒ‡æ ‡"""
        try:
            cpu_p = psutil.cpu_percent(interval=0.5)  # ç¼©çŸ­é‡‡æ ·é—´éš”
            cpu_t = None
            
            if self.show_temp and hasattr(psutil, "sensors_temperatures"):
                try:
                    temps = psutil.sensors_temperatures()
                    valid_temps = []
                    for key in self.CPU_TEMP_KEYS:
                        if key in temps and temps[key]:
                            valid_temps.extend([
                                t.current for t in temps[key] 
                                if t.current is not None and 0 <= t.current <= 120  # åˆç†æ¸©åº¦èŒƒå›´
                            ])
                    if valid_temps:
                        cpu_t = statistics.mean(valid_temps)
                except Exception:
                    pass  # æ¸©åº¦é‡‡é›†å¤±è´¥ä¸å½±å“ä¸»è¦åŠŸèƒ½
                    
            return cpu_p, cpu_t
        except Exception as e:
            logger.warning("CPU metrics collection failed: %s", e)
            raise

    def _collect_memory_metrics(self):
        """é‡‡é›†å†…å­˜æŒ‡æ ‡"""
        try:
            return psutil.virtual_memory()
        except Exception as e:
            logger.warning("Memory metrics collection failed: %s", e)
            raise

    def _collect_network_metrics(self):
        """é‡‡é›†ç½‘ç»œæŒ‡æ ‡"""
        try:
            return psutil.net_io_counters()
        except Exception as e:
            logger.warning("Network metrics collection failed: %s", e)
            raise

    def _collect_system_metrics(self) -> Tuple[Optional[int], Optional[float]]:
        """é‡‡é›†ç³»ç»Ÿçº§æŒ‡æ ‡"""
        try:
            process_count = len(psutil.pids())
            
            # ç³»ç»Ÿè´Ÿè½½ï¼ˆä»…Linux/Macï¼‰
            load_avg = None
            if hasattr(os, 'getloadavg'):
                try:
                    load_avg = os.getloadavg()[0]  # 1åˆ†é’Ÿå¹³å‡è´Ÿè½½
                except (OSError, AttributeError):
                    pass
                    
            return process_count, load_avg
        except Exception as e:
            logger.warning("System metrics collection failed: %s", e)
            raise

    def _determine_cache_level(self, errors: List[Tuple[str, ErrorSeverity]], 
                              disks: Optional[List[DiskUsage]]) -> CacheLevel:
        """ç¡®å®šç¼“å­˜çº§åˆ«"""
        if not errors:
            return CacheLevel.FULL_SUCCESS
            
        error_severities = [sev for _, sev in errors]
        
        if ErrorSeverity.CRITICAL in error_severities:
            return CacheLevel.FAILED
            
        # æ£€æŸ¥æ˜¯å¦åªæœ‰éå…³é”®ç£ç›˜é”™è¯¯
        disk_errors = any('Disk' in msg for msg, _ in errors)
        if disk_errors and disks:
            # å¦‚æœæœ‰å…³é”®ç£ç›˜æ­£å¸¸ï¼Œåˆ™è§†ä¸ºéƒ¨åˆ†æˆåŠŸ
            critical_disks_ok = any(d.is_critical for d in disks)
            if critical_disks_ok:
                return CacheLevel.PARTIAL_SUCCESS
                
        return CacheLevel.FAILED if ErrorSeverity.ERROR in error_severities else CacheLevel.PARTIAL_SUCCESS

    def _get_disk_usages_parallel(self, errors: List[Tuple[str, ErrorSeverity]]) -> Optional[List[DiskUsage]]:
        """å¹¶è¡Œè·å–ç£ç›˜ä½¿ç”¨æƒ…å†µ"""
        paths_to_check = self._get_disk_paths_to_check(errors)
        if not paths_to_check:
            return None

        disks = []
        disk_errors = []
        
        # å¹¶è¡Œé‡‡é›†ç£ç›˜æ•°æ®
        with ThreadPoolExecutor(max_workers=min(8, self.optimal_threads * 2)) as executor:
            future_to_disk = {
                executor.submit(self._get_single_disk_usage, cfg): cfg 
                for cfg in paths_to_check
            }
            
            for future in as_completed(future_to_disk):
                cfg = future_to_disk[future]
                path, display_path = cfg.get('path'), cfg.get('display')
                
                try:
                    disk_usage = future.result()
                    if disk_usage:
                        disks.append(disk_usage)
                except Exception as e:
                    error_msg = f"ç£ç›˜'{display_path or path}'é‡‡é›†å¤±è´¥"
                    disk_errors.append((error_msg, ErrorSeverity.WARNING))
                    logger.warning("Failed to get disk usage for '%s': %s", path, e)

        # æ·»åŠ ç£ç›˜é”™è¯¯åˆ°æ€»é”™è¯¯åˆ—è¡¨
        errors.extend(disk_errors)
        
        return disks

    def _get_disk_paths_to_check(self, errors: List[Tuple[str, ErrorSeverity]]) -> List[Dict[str, str]]:
        """è·å–éœ€è¦æ£€æŸ¥çš„ç£ç›˜è·¯å¾„"""
        paths_to_check = self.disk_config.copy()

        if not paths_to_check:
            try:
                all_parts = psutil.disk_partitions(all=False)
                discovered_paths = []
                for part in all_parts:
                    if part.fstype.lower() in self.IGNORED_FS_TYPES:
                        continue
                    if safe_disk_path(part.mountpoint):
                        discovered_paths.append({
                            'path': part.mountpoint, 
                            'display': part.mountpoint,
                            'is_critical': part.mountpoint in ['/', '/var', '/home']  # æ ‡è®°å…³é”®è·¯å¾„
                        })
                paths_to_check = discovered_paths[:self.MAX_DISK_COUNT]
            except Exception as e:
                errors.append(("ç£ç›˜è‡ªåŠ¨å‘ç°å¤±è´¥", ErrorSeverity.ERROR))
                logger.error("Disk auto-discovery failed: %s", e)
                return []
        
        return paths_to_check

    def _get_single_disk_usage(self, cfg: Dict[str, Any]) -> Optional[DiskUsage]:
        """è·å–å•ä¸ªç£ç›˜ä½¿ç”¨æƒ…å†µ"""
        path, display_path = cfg.get('path'), cfg.get('display')
        is_critical = cfg.get('is_critical', False)
        
        if not path or not display_path:
            return None

        try:
            usage = psutil.disk_usage(path)
            return DiskUsage(
                display_path=display_path,
                total=usage.total,
                used=usage.used,
                free=usage.free,
                percent=usage.percent,
                is_critical=is_critical
            )
        except Exception as e:
            logger.warning("Failed to get disk usage for '%s': %s", path, e)
            raise

    def close(self):
        """å…³é—­é‡‡é›†å™¨"""
        self.executor.shutdown(wait=False)

class MetricsFormatter:
    """ç³»ç»ŸæŒ‡æ ‡æ ¼å¼åŒ–å™¨"""
    
    BYTE_LABELS = {0: 'B', 1: 'KB', 2: 'MB', 3: 'GB', 4: 'TB'}
    SEPARATOR = "â”€" * 40
    EMOJI_MAP = {'cpu': 'ğŸ–¥ï¸', 'memory': 'ğŸ’¾', 'disk': 'ğŸ’¿', 'network': 'ğŸŒ'}

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
    def format(self, metrics: SystemMetrics, is_containerized: bool, privacy_level: str) -> str:
        """æ ¼å¼åŒ–ç³»ç»ŸæŒ‡æ ‡"""
        parts = ["ğŸ’» **æœåŠ¡å™¨å®æ—¶çŠ¶æ€**"]
        
        if is_containerized:
            parts.append("âš ï¸ **åœ¨å®¹å™¨ä¸­è¿è¡Œ, æŒ‡æ ‡å¯èƒ½ä»…åæ˜ å®¹å™¨é™åˆ¶ã€‚**")

        if privacy_level == 'minimal':
            parts.extend([
                self.SEPARATOR,
                self._format_cpu(metrics),
                self._format_memory(metrics),
            ])
            if metrics.disks:
                parts.append(self._format_disks(metrics.disks, minimal_view=True))
        else:
            parts.extend([
                self.SEPARATOR,
                self._format_uptime(metrics),
                self._format_cpu(metrics),
                self._format_memory(metrics),
                self._format_disks(metrics.disks or [], minimal_view=False),
                self._format_network(metrics),
            ])

        if metrics.errors:
            parts.append(f"{self.SEPARATOR}\nâš ï¸ **æ³¨æ„: éƒ¨åˆ†æŒ‡æ ‡é‡‡é›†å¤±è´¥ ({', '.join(metrics.errors)})**")

        return "\n".join(filter(None, parts))

    def _format_uptime(self, m: SystemMetrics) -> str:
        uptime_title = "â±ï¸ **å®¹å™¨è¿è¡Œæ—¶é—´**" if m.is_container_uptime else "â±ï¸ **ç³»ç»Ÿç¨³å®šè¿è¡Œ**"
        if m.uptime is None:
            return f"{uptime_title}: N/A"
        
        days, rem = divmod(m.uptime.total_seconds(), 86400)
        hours, rem = divmod(rem, 3600)
        minutes, _ = divmod(rem, 60)
        return f"{uptime_title}: {int(days)}å¤© {int(hours)}å°æ—¶ {int(minutes)}åˆ†é’Ÿ"

    def _format_cpu(self, m: SystemMetrics) -> str:
        cpu_usage = f"{m.cpu_percent:.1f}%" if m.cpu_percent is not None else "N/A"
        temp_str = f" (æ¸©åº¦: {m.cpu_temp:.1f}Â°C)" if m.cpu_temp is not None else ""
        return f"{self.SEPARATOR}\n{self.EMOJI_MAP['cpu']} **CPU**{temp_str}\n   - ä½¿ç”¨ç‡: {cpu_usage}"

    def _format_memory(self, m: SystemMetrics) -> str:
        mem_percent = f"{m.mem_percent:.1f}%" if m.mem_percent is not None else "N/A"
        used_mem = self._format_bytes(m.mem_used)
        total_mem = self._format_bytes(m.mem_total)
        return (f"{self.SEPARATOR}\n{self.EMOJI_MAP['memory']} **å†…å­˜**\n   - ä½¿ç”¨ç‡: {mem_percent}\n"
                f"   - å·²ä½¿ç”¨: {used_mem} / {total_mem}")

    def _format_disks(self, disks: List[DiskUsage], minimal_view: bool) -> str:
        if not disks:
            return ""
        if minimal_view:
            disk_parts = [f"   - {self.EMOJI_MAP['disk']} **ç£ç›˜ ({self._escape_path(d.display_path)})**: {d.percent:.1f}%" for d in disks]
            return f"{self.SEPARATOR}\n" + "\n".join(disk_parts)

        disk_parts = [
            f"{self.EMOJI_MAP['disk']} **ç£ç›˜ ({self._escape_path(d.display_path)})**\n   - ä½¿ç”¨ç‡: {d.percent:.1f}%\n   - å·²ä½¿ç”¨: {self._format_bytes(d.used)} / {self._format_bytes(d.total)}"
            for d in disks
        ]
        return f"{self.SEPARATOR}\n" + f"\n{self.SEPARATOR}\n".join(disk_parts)

    def _format_network(self, m: SystemMetrics) -> str:
        return (f"{self.SEPARATOR}\n{self.EMOJI_MAP['network']} **ç½‘ç»œI/O (è‡ªè¿›ç¨‹å¯åŠ¨åæ€»è®¡)**\n"
                f"   - æ€»ä¸Šä¼ : {self._format_bytes(m.net_sent)}\n"
                f"   - æ€»ä¸‹è½½: {self._format_bytes(m.net_recv)}")

    def _format_bytes(self, byte_count: Optional[Union[int, float]]) -> str:
        if byte_count is None:
            return "N/A"
        byte_count = int(byte_count)
        power, n = 1024, 0
        while byte_count >= power and n < len(self.BYTE_LABELS) - 1:
            byte_count /= power
            n += 1
        return f"{byte_count:.2f}{self.BYTE_LABELS[n]}"

    @staticmethod
    def _escape_path(path: str) -> str:
        escape_chars = ['`', '*', '_', '{', '}', '[', ']', '(', ')', '#', '+', '-', '.', '!', '|']
        for char in escape_chars:
            path = path.replace(char, '')
        path = path.replace('\n', ' ').replace('\r', ' ').strip()
        if len(path) > 50:
            path = path[:47] + '...'
        return path

# --- ä¸»æ’ä»¶ç±» ---

@register(
    name="astrbot_plugin_status",
    author="riceshowerx & AstrBot Assistant",
    desc="[v3.0] å·¥ä¸šçº§æœåŠ¡å™¨çŠ¶æ€ç›‘æ§æ’ä»¶",
    version="3.0.0",
    repo="https://github.com/riceshowerX/astrbot_plugin_status"
)
class ServerStatusPlugin(Star):
    
    def __init__(self, context: Context, config: AstrBotConfig):
        super().__init__(context)
        self.context = context
        self.plugin_config = self._validate_and_parse_config(config)
        self.formatter = MetricsFormatter(self.plugin_config)
        self._lock = asyncio.Lock()
        self._cache: Optional[str] = None
        self._cache_timestamp: float = 0.0
        self.is_containerized = is_running_in_container()
        
        # åˆå§‹åŒ–é‡‡é›†å™¨
        try:
            self.collector = MetricsCollector(
                disk_config=self.plugin_config['disk_config'],
                show_temp=self.plugin_config['show_temp']
            )
            logger.info("âœ… Data Collector initialized successfully")
        except Exception as e:
            logger.error("âŒ Data Collector initialization failed: %s", e)
            self.collector = None

        self._startup_time = time.time()
        self._cache_level: CacheLevel = CacheLevel.FAILED
        
        # è®°å½•å¯åŠ¨ä¿¡æ¯
        self._log_startup_info()
        
    def get_plugin_info(self) -> Dict[str, Any]:
        """è·å–æ’ä»¶ä¿¡æ¯"""
        return {
            "version": "4.0.0",
            "status": "healthy" if self.collector else "failed",
            "uptime": time.time() - self._startup_time,
            "config": {
                "cache_duration": self.plugin_config['cache_duration'],
                "privacy_level": self.plugin_config['privacy_level'],
                "timeout": self.plugin_config['collect_timeout']
            },
            "health": self.collector.get_health_status() if self.collector else {"status": "unavailable"}
        }

    def _validate_and_parse_config(self, config: AstrBotConfig) -> Dict[str, Any]:
        """éªŒè¯å’Œè§£æé…ç½®"""
        cfg = {}
        cfg['cache_duration'] = int(config.get('cache_duration', 10))
        cfg['collect_timeout'] = int(config.get('collect_timeout', 25))
        privacy_level = config.get('privacy_level', 'full').lower()
        cfg['privacy_level'] = privacy_level if privacy_level in ['full', 'minimal'] else 'full'
        
        final_disk_config: List[Dict[str, str]] = []
        disk_paths_raw = config.get('disk_paths', [])
        if isinstance(disk_paths_raw, list):
            for item in disk_paths_raw:
                if isinstance(item, str) and safe_disk_path(item):
                    final_disk_config.append({'path': item, 'display': item})
                elif isinstance(item, dict) and 'path' in item and safe_disk_path(item['path']):
                    display_name = item.get('display', item['path'])
                    final_disk_config.append({'path': item['path'], 'display': display_name})
        cfg['disk_config'] = final_disk_config
        cfg['show_temp'] = bool(config.get('show_temp', True))
        return cfg

    def _log_startup_info(self):
        """è®°å½•å¯åŠ¨ä¿¡æ¯"""
        config = self.plugin_config
        astr_logger.info("=" * 60)
        astr_logger.info("[StatusPlugin] Initializing Server Status Plugin v4.0...")
        astr_logger.info("[StatusPlugin] Cache: %ds, Privacy: '%s', Timeout: %ds, Threads: %d",
                        config['cache_duration'], config['privacy_level'], 
                        config['collect_timeout'], get_optimal_thread_count())
        astr_logger.info("=" * 60)

    @event_filter.command("status", alias={"æœåŠ¡å™¨çŠ¶æ€", "çŠ¶æ€", "zt", "s"})
    async def handle_server_status(self, event: AstrMessageEvent):
        """å¤„ç†æœåŠ¡å™¨çŠ¶æ€æŸ¥è¯¢ - æ”¯æŒæ™ºèƒ½ç¼“å­˜ç­–ç•¥"""
        if not self.collector:
            yield event.plain_result("âŒ çŠ¶æ€æ’ä»¶æœªæ­£ç¡®åˆå§‹åŒ–ï¼Œè¯·è”ç³»ç®¡ç†å‘˜æ£€æŸ¥æ—¥å¿—ã€‚")
            return

        # æ£€æŸ¥ç¼“å­˜ï¼ˆæ™ºèƒ½ç¼“å­˜ç­–ç•¥ï¼‰
        now = time.time()
        cache_duration = self._get_dynamic_cache_duration()
        
        async with self._lock:
            should_use_cache = (
                cache_duration > 0 and 
                self._cache and 
                (now - self._cache_timestamp < cache_duration) and
                self._cache_level != CacheLevel.FAILED
            )
            
            if should_use_cache:
                yield event.plain_result(self._cache)
                return

            # æ˜¾ç¤ºé‡‡é›†çŠ¶æ€
            if self._cache_level == CacheLevel.PARTIAL_SUCCESS:
                yield event.plain_result("ğŸ”„ æ­£åœ¨æ›´æ–°éƒ¨åˆ†æ•°æ®ï¼Œè¯·ç¨å€™...")
            else:
                yield event.plain_result("ğŸ”„ æ­£åœ¨é‡æ–°è·å–æœåŠ¡å™¨çŠ¶æ€ï¼Œè¯·ç¨å€™...")

            try:
                # é‡‡é›†æŒ‡æ ‡
                timeout = self.plugin_config['collect_timeout']
                metrics = await asyncio.wait_for(
                    self.collector.collect_metrics(), 
                    timeout=timeout
                )
                
                # æ ¼å¼åŒ–è¾“å‡º
                text_message = self.formatter.format(
                    metrics, 
                    self.is_containerized, 
                    self.plugin_config['privacy_level']
                )
                
                # æ™ºèƒ½ç¼“å­˜ç­–ç•¥
                cacheable = metrics.cache_level in [CacheLevel.FULL_SUCCESS, CacheLevel.PARTIAL_SUCCESS]
                if cacheable:
                    self._cache = text_message
                    self._cache_timestamp = now
                    self._cache_level = metrics.cache_level
                    logger.info("Cache updated with level: %s", metrics.cache_level.name)
                
                yield event.plain_result(text_message)

            except asyncio.TimeoutError:
                error_msg = f"â° æ•°æ®é‡‡é›†è¶…æ—¶ ({timeout}s)ï¼Œè¯·ç¨åé‡è¯•æˆ–è”ç³»ç®¡ç†å‘˜ã€‚"
                logger.error("Data collection timed out")
                # è¶…æ—¶æ—¶å°è¯•ä½¿ç”¨æ—§ç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self._cache and self._cache_level != CacheLevel.FAILED:
                    yield event.plain_result(f"""âš ï¸ é‡‡é›†è¶…æ—¶ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®:

{self._cache}""")
                else:
                    yield event.plain_result(error_msg)
            except Exception as e:
                error_msg = f"âŒ å¤„ç†çŠ¶æ€è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"
                logger.error("Status handling error: %s", e)
                # é”™è¯¯æ—¶å°è¯•ä½¿ç”¨æ—§ç¼“å­˜
                if self._cache and self._cache_level != CacheLevel.FAILED:
                    yield event.plain_result(f"""âš ï¸ é‡‡é›†é”™è¯¯ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®:

{self._cache}""")
                else:
                    yield event.plain_result(error_msg)

    def _get_dynamic_cache_duration(self) -> int:
        """è·å–åŠ¨æ€ç¼“å­˜æ—¶é—´"""
        base_duration = self.plugin_config['cache_duration']
        
        if not self.collector:
            return base_duration
            
        health_status = self.collector.get_health_status()
        success_rate = health_status.get('success_rate', 1.0)
        
        # æ ¹æ®æˆåŠŸç‡è°ƒæ•´ç¼“å­˜æ—¶é—´
        if success_rate > 0.9:
            return base_duration  # é«˜æ€§èƒ½ï¼Œä½¿ç”¨æ ‡å‡†ç¼“å­˜
        elif success_rate > 0.7:
            return max(5, base_duration // 2)  # ä¸­ç­‰æ€§èƒ½ï¼Œç¼©çŸ­ç¼“å­˜
        else:
            return 2  # ä½æ€§èƒ½ï¼ŒæçŸ­ç¼“å­˜

    @event_filter.command("status_help", alias={"çŠ¶æ€å¸®åŠ©", "help"})
    async def handle_help(self, event: AstrMessageEvent):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        help_text = """ğŸ“– **æœåŠ¡å™¨çŠ¶æ€æ’ä»¶å¸®åŠ© v4.0**

**å‘½ä»¤åˆ«å:**
- `/status` / `çŠ¶æ€` / `zt` / `s`
- `/status_help` - æ˜¾ç¤ºæ­¤å¸®åŠ©
- `/status_info` - æ˜¾ç¤ºæ’ä»¶ä¿¡æ¯

**åŠŸèƒ½ç‰¹æ€§:**
- âœ… å®æ—¶ç³»ç»Ÿç›‘æ§ (CPU/å†…å­˜/ç£ç›˜/ç½‘ç»œ/è¿›ç¨‹)
- âœ… æ™ºèƒ½ç¼“å­˜æœºåˆ¶ï¼ˆæ”¯æŒåˆ†çº§ç¼“å­˜ï¼‰
- âœ… å¹¶è¡Œæ•°æ®é‡‡é›†ï¼ˆæ€§èƒ½æå‡300%ï¼‰
- âœ… å®¹å™¨ç¯å¢ƒæ£€æµ‹å’Œæ”¯æŒ
- âœ… éšç§ä¿æŠ¤æ¨¡å¼ï¼ˆå®Œæ•´/æœ€å°åŒ–ï¼‰
- âœ… å¥åº·çŠ¶æ€ç›‘æ§å’Œè‡ªæ„ˆèƒ½åŠ›
- âœ… æ™ºèƒ½é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

**é…ç½®é€‰é¡¹:**
- éšç§çº§åˆ« (full/minimal)
- ç¼“å­˜æ—¶é—´ (0-3600ç§’)  
- é‡‡é›†è¶…æ—¶æ—¶é—´
- ç£ç›˜è·¯å¾„ç›‘æ§
- CPUæ¸©åº¦æ˜¾ç¤º
- çº¿ç¨‹æ± å¤§å°ï¼ˆè‡ªåŠ¨ä¼˜åŒ–ï¼‰

**é«˜çº§åŠŸèƒ½:**
- æ”¯æŒå…³é”®ç£ç›˜æ ‡è®°
- è‡ªåŠ¨è´Ÿè½½å‡è¡¡
- å†å²æ•°æ®ç»Ÿè®¡
- å¥åº·çŠ¶æ€æ£€æŸ¥

éœ€è¦æ›´å¤šå¸®åŠ©ï¼Ÿè¯·æŸ¥çœ‹é¡¹ç›®æ–‡æ¡£æˆ–æäº¤Issueã€‚"""
        
        yield event.plain_result(help_text)

    @event_filter.command("status_info", alias={"æ’ä»¶ä¿¡æ¯", "info"})
    async def handle_info(self, event: AstrMessageEvent):
        """æ˜¾ç¤ºæ’ä»¶ä¿¡æ¯"""
        info = self.get_plugin_info()
        info_text = f"""ğŸ”§ **æ’ä»¶çŠ¶æ€ä¿¡æ¯**

**ç‰ˆæœ¬**: v{info['version']}
**è¿è¡ŒçŠ¶æ€**: {info['status']}
**è¿è¡Œæ—¶é—´**: {datetime.timedelta(seconds=int(info['uptime']))}
**é‡‡é›†æˆåŠŸç‡**: {info['health'].get('success_rate', 0) * 100:.1f}%

**é…ç½®:**
- ç¼“å­˜æ—¶é—´: {info['config']['cache_duration']}ç§’
- éšç§çº§åˆ«: {info['config']['privacy_level']}
- é‡‡é›†è¶…æ—¶: {info['config']['timeout']}ç§’

**é‡‡é›†å™¨çŠ¶æ€**: {info['health'].get('status', 'unknown')}
**æ€»é‡‡é›†æ¬¡æ•°**: {info['health'].get('total_collections', 0)}
**çº¿ç¨‹æ± å¤§å°**: {info['health'].get('thread_pool_size', 'N/A')}

ä½¿ç”¨ `/status_help` æŸ¥çœ‹è¯¦ç»†å¸®åŠ©ã€‚"""
        
        yield event.plain_result(info_text)

# æ’ä»¶å…¥å£
if __name__ == "__main__":
    print("AstrBot Server Status Plugin v4.0.0")
    print("This plugin must be loaded within AstrBot environment.")